# Saliency Project

A reinforcement learning study investigating how **salient audiovisual cues** (inspired by electronic gambling machines) influence human learning and decision-making in a restless two-armed bandit task. This repository contains:

- **Experiment code** for running the bandit task (written in Python, using PsychoPy).
- **Analysis scripts** encompassing both model-free (behavioral metrics) and model-based (hierarchical Bayesian RL using Stan) approaches within a unified R workflow.

The work primarily served as the foundation for my Master's thesis in Psychology, focusing on how reward-paired salient stimuli can alter human learning rates, win-stay behavior, and post-reinforcement pauses (PRPs) in a reinforcement learning framework.

---

## Table of Contents

1.  [Quickstart](#quickstart)
2.  [Project Overview](#project-overview)
3.  [Repository Structure](#repository-structure)
4.  [Contact](#contact)

---

## Quickstart
```bash
make setup      # install R deps & CmdStan (once)
make analysis   # run the analysis pipeline end-to-end
```

See `analysis/README.md` and `experiment/README.md` for details.

---

## Project Overview

**Background**
- Modern electronic gambling machines often use bright lights, celebratory sounds, and vivid animations to mark wins—a phenomenon referred to as *salient audiovisual feedback*.
- This study explores whether such cues, when paired with monetary rewards in a bandit task, bias the learning process, measured by reinforcement learning models and behavioral metrics.
- Factors such as **learning rate shifts**, **win-stay tendencies**, and **post-reinforcement pauses** are examined.
- The study also investigates potential moderation by personality traits like sensation seeking or impulsivity using questionnaire data.

**Core Questions**
- Does pairing rewards with salient audiovisual cues change participants' learning rates (specifically, does a salient *win* increase the learning rate compared to non-salient outcomes or losses)?
- Do participants show different patterns of staying with a winning option (win-stay) when the win was associated with salient vs. non-salient feedback?
- Do post-reinforcement pauses (reaction times after receiving a reward) differ following losses, non-salient wins, and salient wins?
- Are these effects moderated by individual differences in sensation seeking or impulsivity?

---

## Repository Structure

```bash
saliency-project/
├─ README.md                      # 1-screen quickstart: run analysis in 2 commands
├─ LICENSE                        # MIT License
├─ CITATION.cff                   # how to cite thesis/software
├─ .gitignore
├─ .editorconfig
├─ .pre-commit-config.yaml
├─ Makefile
├─ docker/
│  └─ Dockerfile
├─ data/
│  ├─ raw/                        # Ouput folder for PsychoPy CSVs
│  │  └─ README_data.md           # ethics/PII policy, schema, source, link to OSF
│  └─ processed/                  # analysis-ready CSV/RDS
├─ experiment/                    # PsychoPy task (Python)
│  ├─ README.md
│  ├─ requirements.txt
│  └─ two_armed_bandit/
├─ analysis/
│  ├─ README.md                   # what run_analysis.R does, outputs
│  ├─ renv.lock                   # R deps lockfile (autogenerated)
│  ├─ .Rprofile
│  ├─ setup.R                     # installs/boots renv
│  ├─ run_analysis.R              # single entrypoint
│  ├─ R/                          # modular R functions
│  │  ├─ io.R
│  │  ├─ preprocess.R
│  │  ├─ rl_models.R
│  │  ├─ behavior_metrics.R
│  │  └─ viz.R
│  ├─ models/                     # Stan files
│  │  ├─ rl_hierarchical.stan
│  │  └─ rl_hierarchical_shift.stan
│  ├─ scripts/
│  └─ outputs/
│     ├─ figs/
│     └─ tables/
└─ .github/
   └─ workflows/
      └─ ci.yml                           # Cursor Rules context file (metadata for AI)
```

---

## Contact

For questions regarding the experiment, analysis, or to request data access, please contact:
-   **Yannik Poth**: [yannikpoth@me.com](mailto:yannikpoth@me.com)
-   Or create an [issue](https://github.com/yannikpoth/saliency-project/issues) in this repository.
