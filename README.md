# Illuminating Decisions: How Salient Reward-Paired Cues Modulate Learning and Choice Behavior

[![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](LICENSE)
[![R 4.5.2](https://img.shields.io/badge/R-4.5.2-blue)](https://www.r-project.org/)
[![Stan](https://img.shields.io/badge/Stan-2.37.0-red)](https://mc-stan.org/)

A reinforcement learning study investigating how **salient audiovisual feedback** (inspired by electronic gambling machines) influences human learning and decision-making in a restless two-armed bandit task.

This repository provides a complete, reproducible scientific workflow including:
- **Experiment code** (Python/PsychoPy) for data collection
- **Analysis pipeline** (R + Stan) with *hierarchical Bayesian RL models* and *Behavioral metrics* (win-stay/lose-shift, post-reinforcement pauses)
- **Docker containerization** for full reproducibility

**Study Focus**: How do flashy lights and sounds paired with monetary rewards bias learning rates, choice behavior, and reaction times? Do individual differences in impulsivity and sensation seeking moderate these effects?

---

## Table of Contents

1. [Project Overview](#project-overview)
2. [Repository Structure](#repository-structure)
3. [Installation & Setup](#installation--setup)
4. [Running the Analysis](#running-the-analysis)
5. [Running the Experiment](#running-the-experiment)
6. [Data](#data)
7. [Key Features](#key-features)
8. [Citation](#citation)
9. [License](#license)
10. [Contact](#contact)

---

## Project Overview

### Background

Modern electronic gambling machines use **salient audiovisual cues** (bright lights, celebratory sounds, vivid animations) to mark wins. This study explores whether such cues, when paired with monetary rewards in a bandit task, systematically bias the learning process.

### Research Questions

1. **Learning Rate Modulation**: Does pairing rewards with salient cues increase learning rates (Î±) compared to non-salient wins or losses?
2. **Win-Stay Behavior**: Do participants show stronger win-stay tendencies following salient wins vs. non-salient wins?
3. **Post-Reinforcement Pauses (PRPs)**: Do reaction times differ following losses, non-salient wins, and salient wins?
4. **Individual Differences**: Are these effects moderated by sensation seeking or impulsivity?

### Task Design

- **Restless two-armed bandit**: Participants repeatedly choose between two options with drifting reward probabilities
- **Variable reinforcement schedule**: Rewards appear probabilistically (not every choice is rewarded)
- **Salient feedback condition**: Some rewards accompanied by flashy audiovisual stimuli, others by minimal feedback
- **Questionnaires**: Barratt Impulsiveness Scale (BIS) and Sensation Seeking Scale (SSS)

---

## Repository Structure

```
.
â”œâ”€â”€ analysis/                       # Analysis pipeline (R + Stan)
â”‚   â”œâ”€â”€ R/                          # Modular R functions
â”‚   â”‚   â”œâ”€â”€ io.R                    # Data loading and output
â”‚   â”‚   â”œâ”€â”€ preprocess.R            # Data cleaning and feature engineering
â”‚   â”‚   â”œâ”€â”€ behavior_metrics.R      # Win-stay, PRPs, model-free metrics
â”‚   â”‚   â”œâ”€â”€ rl_models.R             # RL model fitting wrappers
â”‚   â”‚   â””â”€â”€ viz.R                   # Plotting and visualization
â”‚   â”œâ”€â”€ models/                     # Stan model definitions
â”‚   â”‚   â”œâ”€â”€ rl_hierarchical.stan    # Baseline RL model
â”‚   â”‚   â””â”€â”€ rl_hierarchical_shift.stan  # Model with Î±_shift (salient cue effect)
â”‚   â”œâ”€â”€ outputs/                    # Generated figures and tables
â”‚   â”‚   â”œâ”€â”€ figs/
â”‚   â”‚   â””â”€â”€ tables/
â”‚   â”œâ”€â”€ run_analysis.R              # Main analysis entry point
â”‚   â””â”€â”€ setup.R                     # Environment setup (non-Docker users)
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                        # Original CSV files (read-only)
â”‚   â”‚   â”œâ”€â”€ 01_task_data.csv
â”‚   â”‚   â”œâ”€â”€ 01_questionnaire_data.csv
â”‚   â”‚   â””â”€â”€ ...                     # 44 participants
â”‚   â””â”€â”€ processed/                  # Cleaned and combined data (generated)
â”‚       â”œâ”€â”€ all_task_data.tsv
â”‚       â””â”€â”€ all_questionnaire_data.tsv
â”‚
â”œâ”€â”€ experiment/                     # PsychoPy experiment code
â”‚   â”œâ”€â”€ bandit_task/                # Task implementation
â”‚   â””â”€â”€ requirements.txt            # Python dependencies
â”‚
â”œâ”€â”€ docker/
â”‚   â””â”€â”€ Dockerfile                  # Reproducible R + Stan environment
â”‚
â”œâ”€â”€ .devcontainer/
â”‚   â””â”€â”€ devcontainer.json           # VS Code Dev Container config
â”‚
â”œâ”€â”€ Makefile                        # Convenient pipeline commands
â”œâ”€â”€ renv.lock                       # R package versions (renv)
â”œâ”€â”€ LICENSE                         # MIT License
â””â”€â”€ README.md                       # This file
```

---

## Installation & Setup

### Option 1: Using Docker (Recommended) ğŸ³

**The easiest and most reproducible way to run the analysis.**

#### Prerequisites
- [Docker](https://docs.docker.com/get-docker/) installed
- [VS Code](https://code.visualstudio.com/) with [Dev Containers extension](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers) (optional but recommended)

#### Steps

**A. With VS Code (Interactive Development)**
1. Clone the repository:
   ```bash
   git clone https://github.com/yannikpoth/saliency-project.git
   cd saliency-project
   ```
2. Open in VS Code: `code .`
3. When prompted, click **"Reopen in Container"** (or use Command Palette â†’ "Dev Containers: Reopen in Container")
4. Wait for container build and setup (~5-10 minutes first time)
5. You're ready! Run analysis from the integrated terminal:
   ```bash
   make analysis
   ```

**B. With Docker CLI (No VS Code)**
1. Build the container:
   ```bash
   docker build -t saliency-project -f docker/Dockerfile .
   ```
2. Run the analysis:
   ```bash
   docker run --rm -v $(pwd):/workspace saliency-project Rscript analysis/run_analysis.R
   ```

---

### Option 2: Local R Installation

**For users who prefer native R (no Docker).**

#### Prerequisites
- **R** â‰¥ 4.5.2 ([download](https://cloud.r-project.org/))
- **C++ toolchain** for Stan:
  - **macOS**: Xcode Command Line Tools (`xcode-select --install`)
  - **Windows**: [Rtools](https://cran.r-project.org/bin/windows/Rtools/)
  - **Linux**: `sudo apt install build-essential` (or equivalent)

#### Steps

1. Clone the repository:
   ```bash
   git clone https://github.com/yannikpoth/saliency-project.git
   cd saliency-project
   ```

2. Run the setup script (installs packages via renv):
   ```bash
   Rscript analysis/setup.R
   ```
   *(This may take 10-20 minutes to install all dependencies)*

3. You're ready! See [Running the Analysis](#running-the-analysis) below.

---

## Running the Analysis

Once your environment is set up (Docker or local R), run the full analysis pipeline:

### Using Make (Recommended)
```bash
make analysis
```

### Or directly with R
```bash
Rscript analysis/run_analysis.R
```

### What This Does

The pipeline will:
1. **Load and preprocess data** from `data/raw/`
2. **Compute behavioral metrics** (win-stay, PRPs)
3. **Fit hierarchical Bayesian RL models** using Stan (multiple models for comparison)
4. **Run diagnostics** (convergence checks, posterior predictive checks)
5. **Generate figures and tables** in `analysis/outputs/`

**Expected runtime**: ~10-30 minutes depending on hardware and number of models fitted.

---

## Running the Experiment

To collect **new data** using the PsychoPy task:

### Prerequisites
- Python â‰¥ 3.10
- PsychoPy dependencies (see `experiment/requirements.txt`)

### Steps

1. Navigate to experiment directory:
   ```bash
   cd experiment
   ```

2. Create Python virtual environment:
   ```bash
   python3 -m venv .venv
   source .venv/bin/activate  # On Windows: .venv\Scripts\activate
   ```

3. Install dependencies:
   ```bash
   pip install -r requirements.txt
   ```

4. Run the task:
   ```bash
   python bandit_task/main.py
   ```

5. Output files will be saved to `data/raw/` as:
   - `{participant_id}_task_data.csv` (trial-by-trial choices and rewards)
   - `{participant_id}_questionnaire_data.csv` (BIS and SSS responses)

---

## Data

### Task Data (`*_task_data.csv`)

Trial-level data from the bandit task. Key columns:

| Column | Description |
|--------|-------------|
| `trial` | Trial number (1-215: 15 practice + 200 main) |
| `mode` | `practice` or `main` |
| `choice` | Chosen arm (0 or 1; empty if missed) |
| `reward` | Reward received (0 or 1) |
| `condition` | Feedback type (0=non-salient, 1=salient, 2=missed) |
| `reaction_time` | Response time in seconds |
| `reward_prob_1`, `reward_prob_2` | True reward probabilities for each arm |

### Questionnaire Data (`*_questionnaire_data.csv`)

Participant-level data. Key columns:

| Column | Description |
|--------|-------------|
| `participant_id` | Unique identifier |
| `bis_total` | Total Barratt Impulsiveness Scale score |
| `ss_total` | Total Sensation Seeking Scale score |
| `bis_1`â€“`bis_14` | Individual BIS items |
| `sss_1`â€“`sss_8` | Individual SSS items |

### Privacy Note

The included data (`data/raw/`) contains **sample data** from the actual study. Full datasets may be available upon reasonable request (see [Contact](#contact)).

---

## Key Features

### Hierarchical Bayesian RL Models

Implemented in Stan (`analysis/models/`):

- **Baseline Model** (`rl_hierarchical.stan`):
  - Parameters: Base learning rate (Î±), inverse temperature (Î²)
  - Standard Q-learning with hierarchical structure

- **Salient Cue Model** (`rl_hierarchical_shift.stan`):
  - Adds: Learning rate shift (Î±_shift) for salient wins
  - Tests primary hypothesis: Do salient cues increase learning rates?

- **Future Extensions**:
  - Perseveration parameter (Îº) for choice stickiness
  - Forgetting/decay parameters

### Behavioral Metrics (Model-Free)

- **Win-Stay / Lose-Shift Probabilities**: Computed separately for salient vs. non-salient feedback
- **Post-Reinforcement Pauses (PRPs)**: Median reaction times following different outcomes
- **Statistical Tests**: Paired t-tests, ANOVA, Wilcoxon, with assumption checks

### Robust Workflow

- âœ… **Reproducible**: Docker + renv + version-pinned dependencies
- âœ… **Modular**: Separate R functions for each analysis step
- âœ… **Open Science Ready**: Clear data/code separation, documented pipeline
- âœ… **Diagnostic-Rich**: Convergence checks, PPCs, model comparison (LOO/WAIC)

---

## Citation

If you use this code or data, please cite:

```bibtex
@mastersthesis{poth2025saliency,
  author  = {Poth, Yannik},
  title   = {Illuminating Decisions: How Salient Reward-Paired Cues
             Modulate Learning and Choice Behavior},
  school  = {[Your University]},
  year    = {2025},
  type    = {Master's thesis},
  url     = {https://github.com/yannikpoth/saliency-project}
}
```

See also: [`CITATION.cff`](CITATION.cff)

---

## License

This project is licensed under the **MIT License** - see the [LICENSE](LICENSE) file for details.

**Summary**: You are free to use, modify, and distribute this software for any purpose, including commercial use, provided you include the original copyright notice and disclaimer.

---

## Contact

**Yannik Poth**
ğŸ“§ [yannikpoth@me.com](mailto:yannikpoth@me.com)
ğŸ™ [GitHub Issues](https://github.com/yannikpoth/saliency-project/issues)

For questions about:
- **Data access**: Please reach out via email
- **Technical issues**: Open an issue on GitHub
- **Collaboration**: Email preferred

---

## Acknowledgments

This work was conducted as part of a Master's thesis in Psychology. Special thanks to:
- Supervisors and advisors (names TBD)
- Open source communities: R, Stan, PsychoPy
- Study participants

---

## Additional Resources

- **Stan User's Guide**: https://mc-stan.org/docs/
- **Reinforcement Learning Textbook**: Sutton & Barto (2018)
- **renv Documentation**: https://rstudio.github.io/renv/
- **Docker for R Users**: https://www.rocker-project.org/

---

**Status**: ğŸš§ Active development | ğŸ“Š Data collection complete | ğŸ“ Analysis in progress

*Last updated: November 2025*
